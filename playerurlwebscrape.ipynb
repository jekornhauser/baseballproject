{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "dbd7a643661ad6e3d8a4041f653ed605ed36fe1e24abeb84b653779b055c6359"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[WDM] - Current google-chrome version is 89.0.4389\n",
      "[WDM] - Get LATEST driver version for 89.0.4389\n",
      "[WDM] - Driver [C:\\Users\\jekor\\.wdm\\drivers\\chromedriver\\win32\\89.0.4389.23\\chromedriver.exe] found in cache\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from splinter import Browser\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = [\"ARI\",\"ATL\", \"BAL\", \"BOS\", \"CHC\", \"CHW\", \"CIN\", \"CLE\", \"COL\", \"DET\",\"HOU\", \"KCR\",\"LAD\", \"MIL\", \"MIN\", \"NYM\", \"NYY\", \"OAK\", \"PHI\", \"PIT\", \"SDP\", \"SFG\", \"SEA\", \"STL\", \"TEX\", \"TOR\"]\n",
    "years = [\"2001\",\"2002\",\"2003\",\"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"]\n",
    "laa= [\"LAA\"]\n",
    "laa_years = [\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"]\n",
    "ana = [\"ANA\"]\n",
    "ana_years = [\"2001\",\"2002\",\"2003\",\"2004\"]\n",
    "mia = [\"MIA\"]\n",
    "mia_years = [\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"]\n",
    "fla = [\"FLA\"]\n",
    "fla_years = [\"2001\",\"2002\",\"2003\",\"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\"2010\",\"2011\"]\n",
    "tbr = [\"TBR\"]\n",
    "tbr_years=[\"2008\",\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"]\n",
    "tbd = [\"TBD\"]\n",
    "tbd_years = [\"2001\",\"2002\",\"2003\",\"2004\",\"2005\",\"2006\",\"2007\"]\n",
    "wsn = [\"WSN\"]\n",
    "wsn_years=[\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"]\n",
    "mon = [\"MON\"]\n",
    "mon_years = [\"2001\",\"2002\",\"2003\",\"2004\"]\n",
    "player_url= set()\n",
    "player_url_for_db = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-ddace2dd5c23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'team_batting'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "for x in teams:\n",
    "    for y in years:\n",
    "        URL = \"https://www.baseball-reference.com/teams/\" + x +\"/\" + y +\".shtml\"\n",
    "        browser.visit(URL)\n",
    "        players = []\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        r = soup.find(id='team_batting')\n",
    "        d = r.find_all('a', href=True)\n",
    "        i = 0\n",
    "        for c in d:\n",
    "            link_text = c['href']\n",
    "            if link_text not in player_url:\n",
    "                player_url.add(link_text)\n",
    "                player_url_for_db.append(link_text)\n",
    "            i += 1\n",
    "            if i > 19:\n",
    "                break\n",
    "        time.sleep(5)\n",
    "for x in laa:\n",
    "    for y in laa_years:\n",
    "        URL = \"https://www.baseball-reference.com/teams/\" + x +\"/\" + y +\".shtml\"\n",
    "        browser.visit(URL)\n",
    "        players = []\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        r = soup.find(id='team_batting')\n",
    "        d = r.find_all('a', href=True)\n",
    "        i = 0\n",
    "        for c in d:\n",
    "            link_text = c['href']\n",
    "            if link_text not in player_url:\n",
    "                player_url.add(link_text)\n",
    "                player_url_for_db.append(link_text)\n",
    "            i += 1\n",
    "            if i > 19:\n",
    "                break\n",
    "        time.sleep(5)\n",
    "for x in ana:\n",
    "    for y in ana_years:\n",
    "        URL = \"https://www.baseball-reference.com/teams/\" + x +\"/\" + y +\".shtml\"\n",
    "        browser.visit(URL)\n",
    "        players = []\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        r = soup.find(id='team_batting')\n",
    "        d = r.find_all('a', href=True)\n",
    "        i = 0\n",
    "        for c in d:\n",
    "            link_text = c['href']\n",
    "            if link_text not in player_url:\n",
    "                player_url.add(link_text)\n",
    "                player_url_for_db.append(link_text)\n",
    "            i += 1\n",
    "            if i > 19:\n",
    "                break\n",
    "        time.sleep(5)\n",
    "for x in mia:\n",
    "    for y in mia_years:\n",
    "        URL = \"https://www.baseball-reference.com/teams/\" + x +\"/\" + y +\".shtml\"\n",
    "        browser.visit(URL)\n",
    "        players = []\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        r = soup.find(id='team_batting')\n",
    "        d = r.find_all('a', href=True)\n",
    "        i = 0\n",
    "        for c in d:\n",
    "            link_text = c['href']\n",
    "            if link_text not in player_url:\n",
    "                player_url.add(link_text)\n",
    "                player_url_for_db.append(link_text)\n",
    "            i += 1\n",
    "            if i > 19:\n",
    "                break\n",
    "        time.sleep(5)\n",
    "for x in fla:\n",
    "    for y in fla_years:\n",
    "        URL = \"https://www.baseball-reference.com/teams/\" + x +\"/\" + y +\".shtml\"\n",
    "        browser.visit(URL)\n",
    "        players = []\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        r = soup.find(id='team_batting')\n",
    "        d = r.find_all('a', href=True)\n",
    "        i = 0\n",
    "        for c in d:\n",
    "            link_text = c['href']\n",
    "            if link_text not in player_url:\n",
    "                player_url.add(link_text)\n",
    "                player_url_for_db.append(link_text)\n",
    "            i += 1\n",
    "            if i > 19:\n",
    "                break\n",
    "        time.sleep(5)\n",
    "for x in tbr:\n",
    "    for y in tbr_years:\n",
    "        URL = \"https://www.baseball-reference.com/teams/\" + x +\"/\" + y +\".shtml\"\n",
    "        browser.visit(URL)\n",
    "        players = []\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        r = soup.find(id='team_batting')\n",
    "        d = r.find_all('a', href=True)\n",
    "        i = 0\n",
    "        for c in d:\n",
    "            link_text = c['href']\n",
    "            if link_text not in player_url:\n",
    "                player_url.add(link_text)\n",
    "                player_url_for_db.append(link_text)\n",
    "            i += 1\n",
    "            if i > 19:\n",
    "                break\n",
    "        time.sleep(5)\n",
    "for x in tbd:\n",
    "    for y in tbd_years:\n",
    "        URL = \"https://www.baseball-reference.com/teams/\" + x +\"/\" + y +\".shtml\"\n",
    "        browser.visit(URL)\n",
    "        players = []\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        r = soup.find(id='team_batting')\n",
    "        d = r.find_all('a', href=True)\n",
    "        i = 0\n",
    "        for c in d:\n",
    "            link_text = c['href']\n",
    "            if link_text not in player_url:\n",
    "                player_url.add(link_text)\n",
    "                player_url_for_db.append(link_text)\n",
    "            i += 1\n",
    "            if i > 19:\n",
    "                break\n",
    "        time.sleep(5)\n",
    "for x in wsn:\n",
    "    for y in wsn_years:\n",
    "        URL = \"https://www.baseball-reference.com/teams/\" + x +\"/\" + y +\".shtml\"\n",
    "        browser.visit(URL)\n",
    "        players = []\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        r = soup.find(id='team_batting')\n",
    "        d = r.find_all('a', href=True)\n",
    "        i = 0\n",
    "        for c in d:\n",
    "            link_text = c['href']\n",
    "            if link_text not in player_url:\n",
    "                player_url.add(link_text)\n",
    "                player_url_for_db.append(link_text)\n",
    "            i += 1\n",
    "            if i > 19:\n",
    "                break\n",
    "        time.sleep(5)\n",
    "for x in mon:\n",
    "    for y in mon_years:\n",
    "        URL = \"https://www.baseball-reference.com/teams/\" + x +\"/\" + y +\".shtml\"\n",
    "        browser.visit(URL)\n",
    "        players = []\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        r = soup.find(id='team_batting')\n",
    "        d = r.find_all('a', href=True)\n",
    "        i = 0\n",
    "        for c in d:\n",
    "            link_text = c['href']\n",
    "            if link_text not in player_url:\n",
    "                player_url.add(link_text)\n",
    "                player_url_for_db.append(link_text)\n",
    "            i += 1\n",
    "            if i > 19:\n",
    "                break\n",
    "        time.sleep(5)\n",
    "player_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2367"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "len(player_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {\"Player_URL\": player_url_for_db}\n",
    "player_urls_db = pd.DataFrame(data, columns = [\"Player_URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_urls_db.to_csv(r'C:\\Users\\jekor\\Desktop\\baseball project\\player_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-118-1bc88c1a4083>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-118-1bc88c1a4083>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    '''gfh\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "#Goal of this code was to get a list of player urls of the top 20 batters on every baseball team from 2001 to 2020.  I ran into several problems with the code.  These are my notes taken immediately after finishing this segment.  My counter to only take the first 20 batters from the list works, but is not a clean solution.  A major problem I encountered was that 4 teams have name changes during this time period.  Both my initial and ending code would stop if what I searched for did not have a table to scrape from.  A more clever approach would have been to break out of the code and print out statements saying particular iterations did not work.  More familiarity with the data beforehand would have saved me a good amount of time and led to a simpler and shorter code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}